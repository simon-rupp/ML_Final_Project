{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install appropriate packages and load in datasets from NHANES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1453,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install xport pandas numpy matplotlib scikit-learn pyreadstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1454,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xport import XportReader\n",
    "import os\n",
    "from pyreadstat import read_xport\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import reduce\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.svm import SVC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1455,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants that can be changed\n",
    "MISSINGNESS_THRESHOLD = 20\n",
    "IMPUTATION_STRATEGY = 'mean'\n",
    "KNN_NEIGHBORS = 5\n",
    "BINARY_THRESHOLD = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1456,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DPQ_L.xpt is the dataset containg depression questions\n",
    "with open(\"datasets/questionaire_data/DPQ_L.xpt\", \"rb\") as f:\n",
    "    reader = XportReader(f)\n",
    "    depression_df = pd.DataFrame(reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1457,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SEQN</th>\n",
       "      <th>DPQ010</th>\n",
       "      <th>DPQ020</th>\n",
       "      <th>DPQ030</th>\n",
       "      <th>DPQ040</th>\n",
       "      <th>DPQ050</th>\n",
       "      <th>DPQ060</th>\n",
       "      <th>DPQ070</th>\n",
       "      <th>DPQ080</th>\n",
       "      <th>DPQ090</th>\n",
       "      <th>DPQ100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>130378.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>130379.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>130380.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>130386.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>130387.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6332</th>\n",
       "      <td>142305.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6333</th>\n",
       "      <td>142307.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6334</th>\n",
       "      <td>142308.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6335</th>\n",
       "      <td>142309.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6336</th>\n",
       "      <td>142310.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6337 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          SEQN  DPQ010  DPQ020  DPQ030  DPQ040  DPQ050  DPQ060  DPQ070  \\\n",
       "0     130378.0     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "1     130379.0     0.0     0.0     1.0     0.0     0.0     0.0     0.0   \n",
       "2     130380.0     0.0     0.0     1.0     1.0     0.0     0.0     0.0   \n",
       "3     130386.0     0.0     0.0     0.0     0.0     0.0     1.0     0.0   \n",
       "4     130387.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "...        ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "6332  142305.0     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "6333  142307.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "6334  142308.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "6335  142309.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "6336  142310.0     0.0     0.0     0.0     1.0     0.0     0.0     0.0   \n",
       "\n",
       "      DPQ080  DPQ090  DPQ100  \n",
       "0        NaN     NaN     NaN  \n",
       "1        0.0     0.0     0.0  \n",
       "2        0.0     0.0     0.0  \n",
       "3        0.0     0.0     0.0  \n",
       "4        0.0     0.0     NaN  \n",
       "...      ...     ...     ...  \n",
       "6332     NaN     NaN     NaN  \n",
       "6333     0.0     0.0     NaN  \n",
       "6334     0.0     0.0     NaN  \n",
       "6335     0.0     0.0     NaN  \n",
       "6336     0.0     0.0     0.0  \n",
       "\n",
       "[6337 rows x 11 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(depression_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1458,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# counts of total sums across all questions of last 10 columns only add if value is 1, 2, 3\n",
    "depression_df[\"DPQSUM\"] = depression_df.iloc[:, -10:].apply(lambda x: x[x.isin([1, 2, 3])].sum(), axis=1)\n",
    "\n",
    "# histogram of the sums\n",
    "plt.hist(depression_df[\"DPQSUM\"], bins=range(0, 27, 1))\n",
    "plt.title(\"Total Scores of Depression Questions\")\n",
    "plt.xlabel(\"Total Score\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in the other data sets for prediction and casual understanding of depression scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1459,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Simon\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\sas\\sas_xport.py:475: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[x] = v\n"
     ]
    }
   ],
   "source": [
    "base_dir = \"datasets\"\n",
    "dataframes = {}\n",
    "\n",
    "for root, dirs, files in os.walk(base_dir):\n",
    "    for file in files:\n",
    "        if file.endswith(\".xpt\"):\n",
    "            file_path = os.path.join(root, file)\n",
    "            dataset_name = os.path.splitext(file)[0]\n",
    "            new_df = pd.read_sas(file_path, format='xport', encoding='latin1')\n",
    "            dataframes[dataset_name] = new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overlapping SEQNs among all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1460,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_dataframes = []\n",
    "\n",
    "for name, df in dataframes.items():\n",
    "    missingness = df.isnull().mean() * 100\n",
    "    low_missingness_cols = missingness[missingness <= MISSINGNESS_THRESHOLD].index\n",
    "    filtered_df = df[low_missingness_cols]\n",
    "    filtered_dataframes.append(filtered_df)\n",
    "\n",
    "filtered_dataframes.append(depression_df)\n",
    "\n",
    "# Find common SEQNs across all DataFrames\n",
    "common_seqn = set(filtered_dataframes[0][\"SEQN\"])\n",
    "for df in filtered_dataframes[1:]:\n",
    "    common_seqn = common_seqn.intersection(df[\"SEQN\"])\n",
    "\n",
    "# Filter DataFrames to include only rows with common SEQNs\n",
    "filtered_dataframes = [df[df[\"SEQN\"].isin(common_seqn)] for df in filtered_dataframes]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1461,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SEQN</th>\n",
       "      <th>SDDSRVYR</th>\n",
       "      <th>RIDSTATR</th>\n",
       "      <th>RIAGENDR</th>\n",
       "      <th>RIDAGEYR</th>\n",
       "      <th>RIDRETH1</th>\n",
       "      <th>RIDRETH3</th>\n",
       "      <th>DMDBORN4</th>\n",
       "      <th>DMDHHSIZ</th>\n",
       "      <th>WTINT2YR</th>\n",
       "      <th>...</th>\n",
       "      <th>DPQ020_y</th>\n",
       "      <th>DPQ030_y</th>\n",
       "      <th>DPQ040_y</th>\n",
       "      <th>DPQ050_y</th>\n",
       "      <th>DPQ060_y</th>\n",
       "      <th>DPQ070_y</th>\n",
       "      <th>DPQ080_y</th>\n",
       "      <th>DPQ090_y</th>\n",
       "      <th>DPQ100</th>\n",
       "      <th>DPQSUM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>130378.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>50055.450807</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>130379.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>29087.450605</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>130380.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>80062.674301</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>130386.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>30995.282610</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>130387.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19896.970559</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4766</th>\n",
       "      <td>142300.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>28399.611503</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4767</th>\n",
       "      <td>142303.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>33250.569425</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4768</th>\n",
       "      <td>142307.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>69419.620456</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4769</th>\n",
       "      <td>142308.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>32696.313477</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4770</th>\n",
       "      <td>142309.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>30547.974564</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4771 rows Ã— 97 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          SEQN  SDDSRVYR  RIDSTATR  RIAGENDR  RIDAGEYR  RIDRETH1  RIDRETH3  \\\n",
       "0     130378.0      12.0       2.0       1.0      43.0       5.0       6.0   \n",
       "1     130379.0      12.0       2.0       1.0      66.0       3.0       3.0   \n",
       "2     130380.0      12.0       2.0       2.0      44.0       2.0       2.0   \n",
       "3     130386.0      12.0       2.0       1.0      34.0       1.0       1.0   \n",
       "4     130387.0      12.0       2.0       2.0      68.0       3.0       3.0   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "4766  142300.0      12.0       2.0       2.0      46.0       1.0       1.0   \n",
       "4767  142303.0      12.0       2.0       2.0      69.0       5.0       7.0   \n",
       "4768  142307.0      12.0       2.0       2.0      49.0       4.0       4.0   \n",
       "4769  142308.0      12.0       2.0       1.0      50.0       2.0       2.0   \n",
       "4770  142309.0      12.0       2.0       1.0      40.0       2.0       2.0   \n",
       "\n",
       "      DMDBORN4  DMDHHSIZ      WTINT2YR  ...  DPQ020_y  DPQ030_y  DPQ040_y  \\\n",
       "0          2.0       4.0  50055.450807  ...       NaN       NaN       NaN   \n",
       "1          1.0       2.0  29087.450605  ...       0.0       1.0       0.0   \n",
       "2          2.0       7.0  80062.674301  ...       0.0       1.0       1.0   \n",
       "3          1.0       3.0  30995.282610  ...       0.0       0.0       0.0   \n",
       "4          1.0       1.0  19896.970559  ...       0.0       0.0       0.0   \n",
       "...        ...       ...           ...  ...       ...       ...       ...   \n",
       "4766       2.0       5.0  28399.611503  ...       0.0       1.0       1.0   \n",
       "4767       1.0       2.0  33250.569425  ...       0.0       0.0       0.0   \n",
       "4768       1.0       5.0  69419.620456  ...       0.0       0.0       0.0   \n",
       "4769       2.0       3.0  32696.313477  ...       0.0       0.0       0.0   \n",
       "4770       1.0       5.0  30547.974564  ...       0.0       0.0       0.0   \n",
       "\n",
       "      DPQ050_y  DPQ060_y  DPQ070_y  DPQ080_y  DPQ090_y  DPQ100  DPQSUM  \n",
       "0          NaN       NaN       NaN       NaN       NaN     NaN     0.0  \n",
       "1          0.0       0.0       0.0       0.0       0.0     0.0     1.0  \n",
       "2          0.0       0.0       0.0       0.0       0.0     0.0     2.0  \n",
       "3          0.0       1.0       0.0       0.0       0.0     0.0     1.0  \n",
       "4          0.0       0.0       0.0       0.0       0.0     NaN     0.0  \n",
       "...        ...       ...       ...       ...       ...     ...     ...  \n",
       "4766       0.0       0.0       0.0       0.0       0.0     0.0     3.0  \n",
       "4767       0.0       0.0       0.0       0.0       0.0     NaN     0.0  \n",
       "4768       0.0       0.0       0.0       0.0       0.0     NaN     0.0  \n",
       "4769       0.0       0.0       0.0       0.0       0.0     NaN     0.0  \n",
       "4770       0.0       0.0       0.0       0.0       0.0     NaN     0.0  \n",
       "\n",
       "[4771 rows x 97 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "merged_df = reduce(lambda left, right: pd.merge(left, right, on='SEQN', how='inner'), filtered_dataframes)\n",
    "\n",
    "display(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1462,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix physical activity columns to be standardized\n",
    "\n",
    "unit_conversion = {'D': 365, 'W': 52.14, 'M': 12, 'Y': 1}\n",
    "\n",
    "def standardize_frequency(row, freq_col, unit_col):\n",
    "    if pd.isna(row[unit_col]) or pd.isna(row[freq_col]):\n",
    "        return None  \n",
    "    return row[freq_col] * unit_conversion.get(row[unit_col], 1)\n",
    "\n",
    "merged_df['PAD790_annualized'] = merged_df.apply(\n",
    "    lambda row: standardize_frequency(row, 'PAD790Q', 'PAD790U'), axis=1)\n",
    "\n",
    "merged_df['PAD810_annualized'] = merged_df.apply(\n",
    "    lambda row: standardize_frequency(row, 'PAD810Q', 'PAD810U'), axis=1)\n",
    "\n",
    "# merged_df['PAD790_total_minutes'] = merged_df['PAD790_annualized'] * merged_df['PAD800']\n",
    "# merged_df['PAD810_total_minutes'] = merged_df['PAD810_annualized'] * merged_df['PAD820']\n",
    "\n",
    "# merged_df = merged_df.drop(columns=['PAD800', 'PAD820'])\n",
    "merged_df = merged_df.drop(columns=['PAD790Q', 'PAD790U', 'PAD810Q', 'PAD810U'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1463,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Simon\\AppData\\Local\\Temp\\ipykernel_42896\\1654022778.py:3: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  merged_df[col] = pd.to_numeric(merged_df[col].str.replace(r'\\D', ''), errors='coerce')\n"
     ]
    }
   ],
   "source": [
    "# Deal with non-numeric columns\n",
    "for col in ['SLQ300', 'SLQ310', 'SLQ320', 'SLQ330']:\n",
    "    merged_df[col] = pd.to_numeric(merged_df[col].str.replace(r'\\D', ''), errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1464,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IMPUTATION_STRATEGY == 'KNN':\n",
    "    imputer = KNNImputer(n_neighbors=KNN_NEIGHBORS)\n",
    "else:\n",
    "    imputer = SimpleImputer(strategy=f'{IMPUTATION_STRATEGY}')\n",
    "\n",
    "merged_df = pd.DataFrame(imputer.fit_transform(merged_df), columns=merged_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1465,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need just one column for depression score not individual questions, but keep dpq sum column\n",
    "merged_df = merged_df.drop(columns=merged_df.columns[merged_df.columns.str.startswith(\"DPQ\") & ~merged_df.columns.str.startswith(\"DPQSUM\")])\n",
    "\n",
    "# dpqsum at end\n",
    "merged_df = merged_df[[c for c in merged_df if c != 'DPQSUM'] + ['DPQSUM']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary Target Distribution:\n",
      "0    4099\n",
      "1     672\n",
      "Name: DEPRESSED, dtype: int64\n",
      "Continuous Target Distribution:\n",
      "1    3235\n",
      "2     864\n",
      "3     411\n",
      "4     159\n",
      "5     102\n",
      "Name: DEPRESSION_LEVEL, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# split up target (dpqsum) two different ways: binary (depressed or not) and continuous (minimal, mild, moderate, moderately severe, severe)\n",
    "# Binary: 0-9 not depressed, 10-27 depressed\n",
    "# Continuous: 0-4 minimal, 5-9 mild, 10-14 moderate, 15-19 moderately severe, 20-27 severe\n",
    "\n",
    "merged_df[\"DEPRESSED\"] = merged_df[\"DPQSUM\"].apply(lambda x: 1 if x >= BINARY_THRESHOLD else 0)\n",
    "merged_df[\"DEPRESSION_LEVEL\"] = merged_df[\"DPQSUM\"].apply(lambda x: 1 if x < 5 else 2 if x < 10 else 3 if x < 15 else 4 if x < 20 else 5)\n",
    "\n",
    "# distribution of these targets\n",
    "print(\"Binary Target Distribution:\")\n",
    "print(merged_df[\"DEPRESSED\"].value_counts())\n",
    "print(\"Continuous Target Distribution:\")\n",
    "print(merged_df[\"DEPRESSION_LEVEL\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1467,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Binary Target Distribution:\n",
      "0    672\n",
      "1    672\n",
      "Name: DEPRESSED, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "not_depressed = merged_df[merged_df['DEPRESSED'] == 0]\n",
    "depressed = merged_df[merged_df['DEPRESSED'] == 1]\n",
    "\n",
    "# randomly remove some not depressed samples to balance the dataset\n",
    "not_depressed_downsampled = resample(not_depressed, replace=False, n_samples=len(depressed), random_state=42)\n",
    "\n",
    "balanced_df = pd.concat([not_depressed_downsampled, depressed])\n",
    "balanced_df = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(\"New Binary Target Distribution:\")\n",
    "print(balanced_df['DEPRESSED'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1468,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SEQN</th>\n",
       "      <th>SDDSRVYR</th>\n",
       "      <th>RIDSTATR</th>\n",
       "      <th>RIAGENDR</th>\n",
       "      <th>RIDAGEYR</th>\n",
       "      <th>RIDRETH1</th>\n",
       "      <th>RIDRETH3</th>\n",
       "      <th>DMDBORN4</th>\n",
       "      <th>DMDHHSIZ</th>\n",
       "      <th>WTINT2YR</th>\n",
       "      <th>...</th>\n",
       "      <th>SMQ020</th>\n",
       "      <th>SMAQUEX2</th>\n",
       "      <th>WHD010</th>\n",
       "      <th>WHD020</th>\n",
       "      <th>WHD050</th>\n",
       "      <th>WHQ070</th>\n",
       "      <th>PAD790_annualized</th>\n",
       "      <th>PAD810_annualized</th>\n",
       "      <th>DEPRESSED</th>\n",
       "      <th>DEPRESSION_LEVEL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>137537.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>26712.638364</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.042800e+02</td>\n",
       "      <td>5.214000e+01</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>134296.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>25387.930391</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>184.0</td>\n",
       "      <td>184.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.649800e+02</td>\n",
       "      <td>5.397605e-79</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>135674.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>23370.552508</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>155.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.564200e+02</td>\n",
       "      <td>1.200000e+01</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>136946.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>33050.051273</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>178.0</td>\n",
       "      <td>178.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.085600e+02</td>\n",
       "      <td>5.397605e-79</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>138395.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>36965.767759</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.042800e+02</td>\n",
       "      <td>5.397605e-79</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1339</th>\n",
       "      <td>138011.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>16076.247553</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>185.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.214000e+01</td>\n",
       "      <td>5.397605e-79</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1340</th>\n",
       "      <td>138517.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24123.683391</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.214000e+01</td>\n",
       "      <td>1.200000e+01</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1341</th>\n",
       "      <td>141264.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>16459.015513</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>158.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.564200e+02</td>\n",
       "      <td>1.564200e+02</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1342</th>\n",
       "      <td>133882.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>44676.554600</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.397605e-79</td>\n",
       "      <td>5.397605e-79</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1343</th>\n",
       "      <td>138439.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>13190.519097</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.649800e+02</td>\n",
       "      <td>5.397605e-79</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1344 rows Ã— 77 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          SEQN  SDDSRVYR  RIDSTATR  RIAGENDR  RIDAGEYR  RIDRETH1  RIDRETH3  \\\n",
       "0     137537.0      12.0       2.0       2.0      37.0       1.0       1.0   \n",
       "1     134296.0      12.0       2.0       1.0      69.0       3.0       3.0   \n",
       "2     135674.0      12.0       2.0       2.0      25.0       3.0       3.0   \n",
       "3     136946.0      12.0       2.0       1.0      67.0       3.0       3.0   \n",
       "4     138395.0      12.0       2.0       2.0      39.0       3.0       3.0   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "1339  138011.0      12.0       2.0       2.0      68.0       3.0       3.0   \n",
       "1340  138517.0      12.0       2.0       1.0      30.0       3.0       3.0   \n",
       "1341  141264.0      12.0       2.0       2.0      50.0       2.0       2.0   \n",
       "1342  133882.0      12.0       2.0       2.0      65.0       4.0       4.0   \n",
       "1343  138439.0      12.0       2.0       1.0      69.0       3.0       3.0   \n",
       "\n",
       "      DMDBORN4  DMDHHSIZ      WTINT2YR  ...  SMQ020  SMAQUEX2  WHD010  WHD020  \\\n",
       "0          1.0       4.0  26712.638364  ...     2.0       1.0    65.0   175.0   \n",
       "1          1.0       2.0  25387.930391  ...     2.0       1.0    63.0   184.0   \n",
       "2          1.0       4.0  23370.552508  ...     2.0       1.0    64.0   155.0   \n",
       "3          1.0       2.0  33050.051273  ...     1.0       1.0    68.0   178.0   \n",
       "4          1.0       1.0  36965.767759  ...     2.0       1.0    70.0   160.0   \n",
       "...        ...       ...           ...  ...     ...       ...     ...     ...   \n",
       "1339       1.0       4.0  16076.247553  ...     2.0       1.0    66.0   195.0   \n",
       "1340       1.0       1.0  24123.683391  ...     1.0       1.0    73.0   170.0   \n",
       "1341       1.0       2.0  16459.015513  ...     1.0       1.0    59.0   158.0   \n",
       "1342       1.0       3.0  44676.554600  ...     2.0       1.0    62.0   200.0   \n",
       "1343       1.0       4.0  13190.519097  ...     2.0       1.0    68.0   170.0   \n",
       "\n",
       "      WHD050  WHQ070  PAD790_annualized  PAD810_annualized  DEPRESSED  \\\n",
       "0      165.0     1.0       1.042800e+02       5.214000e+01          0   \n",
       "1      184.0     1.0       3.649800e+02       5.397605e-79          0   \n",
       "2      180.0     1.0       1.564200e+02       1.200000e+01          1   \n",
       "3      178.0     2.0       2.085600e+02       5.397605e-79          0   \n",
       "4      168.0     1.0       1.042800e+02       5.397605e-79          0   \n",
       "...      ...     ...                ...                ...        ...   \n",
       "1339   185.0     1.0       5.214000e+01       5.397605e-79          1   \n",
       "1340   170.0     2.0       5.214000e+01       1.200000e+01          1   \n",
       "1341   168.0     1.0       1.564200e+02       1.564200e+02          1   \n",
       "1342   165.0     1.0       5.397605e-79       5.397605e-79          1   \n",
       "1343   180.0     2.0       3.649800e+02       5.397605e-79          1   \n",
       "\n",
       "      DEPRESSION_LEVEL  \n",
       "0                    1  \n",
       "1                    1  \n",
       "2                    3  \n",
       "3                    1  \n",
       "4                    2  \n",
       "...                ...  \n",
       "1339                 5  \n",
       "1340                 3  \n",
       "1341                 3  \n",
       "1342                 4  \n",
       "1343                 4  \n",
       "\n",
       "[1344 rows x 77 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# remove dpqsum column\n",
    "balanced_df = balanced_df.drop(columns=\"DPQSUM\")\n",
    "\n",
    "display(balanced_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1469,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = balanced_df.drop(columns=[\"DEPRESSED\", \"DEPRESSION_LEVEL\"])\n",
    "y_binary = balanced_df[\"DEPRESSED\"]\n",
    "y_continuous = balanced_df[\"DEPRESSION_LEVEL\"]\n",
    "X_train, X_test, y_binary_train, y_binary_test, y_continuous_train, y_continuous_test = train_test_split(X, y_binary, y_continuous, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1470,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# identify NaN values in the dataset\n",
    "print(X_train.isna().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1471,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try Random Forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1472,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary Target Accuracy: 0.7769516728624535\n",
      "Binary Target Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.77      0.79       144\n",
      "           1       0.75      0.78      0.77       125\n",
      "\n",
      "    accuracy                           0.78       269\n",
      "   macro avg       0.78      0.78      0.78       269\n",
      "weighted avg       0.78      0.78      0.78       269\n",
      "\n"
     ]
    }
   ],
   "source": [
    "randomForest = RandomForestClassifier(random_state=42, max_depth=3, n_estimators=100)\n",
    "randomForest.fit(X_train, y_binary_train)\n",
    "\n",
    "y_binary_pred = randomForest.predict(X_test)\n",
    "print(\"Binary Target Accuracy:\", accuracy_score(y_binary_test, y_binary_pred))\n",
    "print(\"Binary Target Classification Report:\")\n",
    "print(classification_report(y_binary_test, y_binary_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1473,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Mean Score: 0.7366198746046718\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(randomForest, X, y_binary, cv=5)\n",
    "print(\"Random Forest Mean Score:\", scores.mean())\n",
    "results[\"Random Forest\"] = scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1474,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Mean Score: 0.7172529545580646\n"
     ]
    }
   ],
   "source": [
    "lr_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "lr_scores = cross_val_score(lr_model, X_scaled, y_binary, cv=5)\n",
    "print(\"Logistic Regression Mean Score:\", lr_scores.mean())\n",
    "results[\"Logistic Regression\"] = lr_scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1475,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Mean Score: 0.7165205570659713\n"
     ]
    }
   ],
   "source": [
    "svm = SVC(kernel='rbf', random_state=42)\n",
    "cv_scores = cross_val_score(svm, X_scaled, y_binary, cv=5)\n",
    "print(\"SVM Mean Score:\", cv_scores.mean())\n",
    "results[\"SVM\"] = cv_scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1476,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Mean Score: 0.7514980857792821\n"
     ]
    }
   ],
   "source": [
    "xgboost_model = XGBClassifier()\n",
    "scores = cross_val_score(xgboost_model, X.to_numpy(), y_binary, cv=5, error_score='raise')\n",
    "print(\"XGBoost Mean Score:\", scores.mean())\n",
    "results[\"XGBoost\"] = scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1477,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 538, number of negative: 537\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000556 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3750\n",
      "[LightGBM] [Info] Number of data points in the train set: 1075, number of used features: 72\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500465 -> initscore=0.001860\n",
      "[LightGBM] [Info] Start training from score 0.001860\n",
      "[LightGBM] [Info] Number of positive: 538, number of negative: 537\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000561 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3738\n",
      "[LightGBM] [Info] Number of data points in the train set: 1075, number of used features: 72\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500465 -> initscore=0.001860\n",
      "[LightGBM] [Info] Start training from score 0.001860\n",
      "[LightGBM] [Info] Number of positive: 537, number of negative: 538\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000781 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3712\n",
      "[LightGBM] [Info] Number of data points in the train set: 1075, number of used features: 72\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Info] Number of positive: 537, number of negative: 538\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000403 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3716\n",
      "[LightGBM] [Info] Number of data points in the train set: 1075, number of used features: 72\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Info] Number of positive: 538, number of negative: 538\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000434 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3744\n",
      "[LightGBM] [Info] Number of data points in the train set: 1076, number of used features: 72\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "LightGBM Mean Score: 0.7462908505798147\n"
     ]
    }
   ],
   "source": [
    "lgbm = LGBMClassifier(random_state=42)\n",
    "cv_scores = cross_val_score(lgbm, X, y_binary, cv=5)\n",
    "print(\"LightGBM Mean Score:\", cv_scores.mean())\n",
    "results[\"LightGBM\"] = cv_scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1478,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 538, number of negative: 537\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000565 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3750\n",
      "[LightGBM] [Info] Number of data points in the train set: 1075, number of used features: 72\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500465 -> initscore=0.001860\n",
      "[LightGBM] [Info] Start training from score 0.001860\n",
      "[LightGBM] [Info] Number of positive: 431, number of negative: 429\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000612 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3606\n",
      "[LightGBM] [Info] Number of data points in the train set: 860, number of used features: 72\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501163 -> initscore=0.004651\n",
      "[LightGBM] [Info] Start training from score 0.004651\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 431, number of negative: 429\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000574 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3589\n",
      "[LightGBM] [Info] Number of data points in the train set: 860, number of used features: 72\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501163 -> initscore=0.004651\n",
      "[LightGBM] [Info] Start training from score 0.004651\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 430, number of negative: 430\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000591 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3604\n",
      "[LightGBM] [Info] Number of data points in the train set: 860, number of used features: 72\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 430, number of negative: 430\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000674 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3596\n",
      "[LightGBM] [Info] Number of data points in the train set: 860, number of used features: 72\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 430, number of negative: 430\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000704 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3608\n",
      "[LightGBM] [Info] Number of data points in the train set: 860, number of used features: 72\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 538, number of negative: 537\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000530 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3738\n",
      "[LightGBM] [Info] Number of data points in the train set: 1075, number of used features: 72\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500465 -> initscore=0.001860\n",
      "[LightGBM] [Info] Start training from score 0.001860\n",
      "[LightGBM] [Info] Number of positive: 431, number of negative: 429\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000766 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3597\n",
      "[LightGBM] [Info] Number of data points in the train set: 860, number of used features: 72\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501163 -> initscore=0.004651\n",
      "[LightGBM] [Info] Start training from score 0.004651\n",
      "[LightGBM] [Info] Number of positive: 431, number of negative: 429\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000882 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3580\n",
      "[LightGBM] [Info] Number of data points in the train set: 860, number of used features: 72\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501163 -> initscore=0.004651\n",
      "[LightGBM] [Info] Start training from score 0.004651\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 430, number of negative: 430\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000843 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3590\n",
      "[LightGBM] [Info] Number of data points in the train set: 860, number of used features: 72\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 430, number of negative: 430\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001374 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3592\n",
      "[LightGBM] [Info] Number of data points in the train set: 860, number of used features: 72\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 430, number of negative: 430\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000743 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3612\n",
      "[LightGBM] [Info] Number of data points in the train set: 860, number of used features: 72\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 537, number of negative: 538\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001259 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3712\n",
      "[LightGBM] [Info] Number of data points in the train set: 1075, number of used features: 72\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Info] Number of positive: 430, number of negative: 430\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000705 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3580\n",
      "[LightGBM] [Info] Number of data points in the train set: 860, number of used features: 72\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 430, number of negative: 430\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000809 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3575\n",
      "[LightGBM] [Info] Number of data points in the train set: 860, number of used features: 72\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 430, number of negative: 430\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000743 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3577\n",
      "[LightGBM] [Info] Number of data points in the train set: 860, number of used features: 72\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 429, number of negative: 431\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000521 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3571\n",
      "[LightGBM] [Info] Number of data points in the train set: 860, number of used features: 72\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.498837 -> initscore=-0.004651\n",
      "[LightGBM] [Info] Start training from score -0.004651\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 429, number of negative: 431\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000959 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3584\n",
      "[LightGBM] [Info] Number of data points in the train set: 860, number of used features: 72\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.498837 -> initscore=-0.004651\n",
      "[LightGBM] [Info] Start training from score -0.004651\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 537, number of negative: 538\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000599 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3716\n",
      "[LightGBM] [Info] Number of data points in the train set: 1075, number of used features: 72\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499535 -> initscore=-0.001860\n",
      "[LightGBM] [Info] Start training from score -0.001860\n",
      "[LightGBM] [Info] Number of positive: 430, number of negative: 430\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000737 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3587\n",
      "[LightGBM] [Info] Number of data points in the train set: 860, number of used features: 72\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 430, number of negative: 430\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000748 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3581\n",
      "[LightGBM] [Info] Number of data points in the train set: 860, number of used features: 72\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 430, number of negative: 430\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000629 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3585\n",
      "[LightGBM] [Info] Number of data points in the train set: 860, number of used features: 72\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 429, number of negative: 431\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000619 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3574\n",
      "[LightGBM] [Info] Number of data points in the train set: 860, number of used features: 72\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.498837 -> initscore=-0.004651\n",
      "[LightGBM] [Info] Start training from score -0.004651\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 429, number of negative: 431\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000763 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3581\n",
      "[LightGBM] [Info] Number of data points in the train set: 860, number of used features: 72\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.498837 -> initscore=-0.004651\n",
      "[LightGBM] [Info] Start training from score -0.004651\n",
      "[LightGBM] [Info] Number of positive: 538, number of negative: 538\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000616 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3744\n",
      "[LightGBM] [Info] Number of data points in the train set: 1076, number of used features: 72\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 430, number of negative: 430\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000677 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3606\n",
      "[LightGBM] [Info] Number of data points in the train set: 860, number of used features: 72\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 431, number of negative: 430\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000810 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3598\n",
      "[LightGBM] [Info] Number of data points in the train set: 861, number of used features: 72\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500581 -> initscore=0.002323\n",
      "[LightGBM] [Info] Start training from score 0.002323\n",
      "[LightGBM] [Info] Number of positive: 431, number of negative: 430\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000670 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3593\n",
      "[LightGBM] [Info] Number of data points in the train set: 861, number of used features: 72\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500581 -> initscore=0.002323\n",
      "[LightGBM] [Info] Start training from score 0.002323\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 430, number of negative: 431\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000704 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3597\n",
      "[LightGBM] [Info] Number of data points in the train set: 861, number of used features: 72\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499419 -> initscore=-0.002323\n",
      "[LightGBM] [Info] Start training from score -0.002323\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 430, number of negative: 431\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000658 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3589\n",
      "[LightGBM] [Info] Number of data points in the train set: 861, number of used features: 72\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499419 -> initscore=-0.002323\n",
      "[LightGBM] [Info] Start training from score -0.002323\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Stacked Model Mean Score: 0.7559701492537314\n"
     ]
    }
   ],
   "source": [
    "meta_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "stacked_model = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('rf', randomForest),\n",
    "        # ('lr', lr_model),\n",
    "        ('xgb', xgboost_model),\n",
    "        ('lgbm', lgbm),\n",
    "        # ('svm', svm)\n",
    "    ],\n",
    "    final_estimator=meta_model,\n",
    "    cv=5\n",
    ")\n",
    "\n",
    "stacked_scores = cross_val_score(stacked_model, X, y_binary, cv=5)\n",
    "print(\"Stacked Model Mean Score:\", stacked_scores.mean())\n",
    "results[\"Stacked Model\"] = stacked_scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1479,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Mean Score: 0.7366198746046718\n",
      "Logistic Regression Mean Score: 0.7172529545580646\n",
      "SVM Mean Score: 0.7165205570659713\n",
      "XGBoost Mean Score: 0.7514980857792821\n",
      "LightGBM Mean Score: 0.7462908505798147\n",
      "Stacked Model Mean Score: 0.7559701492537314\n"
     ]
    }
   ],
   "source": [
    "for model, score in results.items():\n",
    "    print(f\"{model} Mean Score: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install econml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
